---
title: "Vol-Chapter"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Volatility

```{r setup, message = FALSE}
#webshot::install_phantomjs()
library(tidyquant)
library(tidyverse)
library(timetk)
library(tibbletime)
library(highcharter)

knitr::opts_chunk$set(message=FALSE, warning=FALSE)

load("~/reproducible-fin-chapters/book-data.Rdata")
```

Welcome to our chapter focused on portfolio volatility, variance and standard deviation. I realize that it's a lot more fun to ~~fantasize about~~ analyze stock returns which is why television shows and websites constantly update the daily market returns and give them snazzy green and red colors. But good ol'volatility is quite important in its own right, especially to finance geeks, aspiring finance geeks and institutional investors. If you are, might become, or might ever work with/for any of those, this chapter should at least serve as a jumping off point.

A quick word of warning that this chapter begins at the beginning with portfolio standard deviation and builds up to more complex work. R users with experience in the world of volatility may wish to skip a few sections or head straight to the Shiny sections.  That said, I would humbly offer a couple of benefits to the R code that awaits us. 

First, volatility is important, possibly more important than returns. I don't think any investment professional looks back on hours spent pondering volatility as a waste of time. 

Second, as always, we have an eye on making our work reproducible and reusable. We'll make it exceedingly clear how we derive our final data visualizations on portfolio volatility. It's a good template for other visualization derivations, even if standard deviation is old hat for you.


## Introduction to Volatility

We will start with the textbook equation for the standard deviation of a multi-asset portfolio. 

Here is the equation for sd: 

$$Standard~Deviation=\sqrt{\sum_{t=1}^n (x_i-\overline{x})^2/n}$$


- First, we assign the weights of each asset.
- Then, we isolate and assign returns of each asset. 
- Next, we plug those weights and returns into the equation for portfolio standard deviation, which involves the following:
- Take the weight squared of each asset times it's variance and sum those weighted variance terms.
- Then we take the covariance of each asset pair, multiplied by 2 times the weight of the first asset times the weight of the second asset. 
- Sum together the covariance terms and the weighted variance terms. 
- This gives us the portfolio variance. 
- Then take the square root to get the standard deviation. 


```{r By Hand Std Dev}

w_1 <- w[1]
w_2 <- w[2]
w_3 <- w[3]
w_4 <- w[4]
w_5 <- w[5]

asset1 <- asset_returns_xts[,1]
asset2 <- asset_returns_xts[,2]
asset3 <- asset_returns_xts[,3]
asset4 <- asset_returns_xts[,4]
asset5 <- asset_returns_xts[,5]
 

sd_by_hand <- 
  # Important, don't forget to take the square root! 
  sqrt(
  # Our weighted variance terms.  
  (w_1^2 * var(asset1)) + (w_2^2 * var(asset2)) + (w_3^2 * var(asset3)) +
  (w_4^2 * var(asset4)) + (w_5^2 * var(asset5)) +
  # Our weighted covariance terms
  (2 * w_1 * w_2 * cov(asset1, asset2)) +  
  (2 * w_1 * w_3 * cov(asset1, asset3)) +
  (2 * w_1 * w_4 * cov(asset1, asset4)) +
  (2 * w_1 * w_5 * cov(asset1, asset5)) +
  (2 * w_2 * w_3 * cov(asset2, asset3)) +
  (2 * w_2 * w_4 * cov(asset2, asset4)) +
  (2 * w_2 * w_5 * cov(asset2, asset5)) +
  (2 * w_3 * w_4 * cov(asset3, asset4)) +
  (2 * w_3 * w_5 * cov(asset3, asset5)) +
  (2 * w_4 * w_5 * cov(asset4, asset5))
  )

# I want to print the percentage, so multiply by 100.
sd_by_hand_percent <- round(sd_by_hand * 100, 2)

```

Writing that equation out was painful but at least we won't be forgetting it any time soon. Our result is a monthly portfolio returns standard deviation of `r sd_by_hand_percent`%.  

Now let's turn to the less verbose matrix algebra path and confirm that we get the same result. 

First, we will build a covariance matrix of returns using the `cov()` function. 

```{r}

# Build the covariance matrix. 
covariance_matrix <- cov(asset_returns_xts)
covariance_matrix
```

Have a look at the covariance matrix. 

AGG, the US bond ETF, has a negative or very low covariance with the other ETFs and it should make a nice volatility dampener.  Interestingly, the covariance between EEM and EFA is quite low as well.  Our painstakingly written-out equation above is a good reminder of how low covariances affect total portfolio standard deviation.

Back to our calculation, let's take the square root of the transpose of the weights vector times the covariance matrix times the weights vector. To perform matrix multiplcation, we use `%*%`.

```{r}
sd_matrix_algebra <- sqrt(t(w) %*% covariance_matrix %*% w)

sd_matrix_algebra_percent <- round(sd_matrix_algebra * 100, 2)

sd_by_hand_percent
sd_matrix_algebra_percent
```

Have a look at our two standard deviation calculations Thankfully, these return the same result so we don't have to sort through the by-hand equation again. 

And, finally, we can use the built-in `StdDev()` function from the `PerformanceAnalytics` package. It takes two arguments, returns and weights: `StdDev(asset_returns_xts, weights = w)`.

```{r}

portfolio_sd_xts_builtin <- StdDev(asset_returns_xts, weights = w)


portfolio_sd_xts_builtin_percent <- round(portfolio_sd_xts_builtin * 100, 2)
```

We now have: 

```{r}
sd_by_hand_percent
sd_matrix_algebra_percent
portfolio_sd_xts_builtin_percent

```


Now let's head to the tidyverse and explore the code flow. We will start with the portfolio returns object `portfolio_returns_tq_rebalanced_monthly` and we want to calculate the lifetime standard deviation. We use the `summarise()` function from `dplyr` and then the base function `sd()`.  We will also perform the calculation with our own equation `sqrt(sum((returns - mean(returns))^2)/(nrow(.)-1)))`. 

```{r}
portfolio_sd_tidy_builtin_percent <-
  portfolio_returns_tq_rebalanced_monthly %>% 
  summarise(sd = sd(returns),
            sd_byhand = sqrt(sum((returns - mean(returns))^2)/(nrow(.)-1))) %>% 
  mutate(sd = round(sd, 4) * 100,
         sd_byhand = round(sd_byhand, 4) * 100)  %>%
  select(sd, sd_byhand)

portfolio_sd_tidy_builtin_percent  
```
 
 Here are our standard deviation calculations thus far:

- by-hand calculation = `r sd_by_hand_percent`%
- matrix algebra calculation = `r sd_matrix_algebra_percent`%
- xts built in function calculation = `r portfolio_sd_xts_builtin_percent`%
- tidy built in in function calculation = `r portfolio_sd_tidy_builtin_percent$sd`%
- tidy by hand calculation = `r portfolio_sd_tidy_builtin_percent$sd_byhand`%

That was quite a lot of work to confirm that 5 calculations are equal to each other but there are a few benefits.

First, while it was tedious, we should feel comfortable with calculating portfolio standard deviations in various ways and starting from different object types.

More importantly, as our work gets more complicated and we build custom functions, we'll want to rely on the built-in `StdDev` function and the built-in tidy workflow.  Our tedious work here, where we used different flows and functions by choice, will facilitate our future work when we need to solve harder coding challenges.  

Beyond helping our own work, it can help our colleagues to understand the more complex work if we can point back to this starter code flow on standard deviation.  

# Visualize 

```{r}
library(scales)
portfolio_returns_tq_rebalanced_monthly %>%

  ggplot(aes(x = date, y = returns)) + 
  
  geom_point(color = "cornflowerblue") +
  scale_x_date(breaks = pretty_breaks(n = 8)) 
```

Hard to pick up much here though we can notice that the year 2017 had consistently positive monthly returns. Let's add some more indicators. It might be nice to have a different color for any monthly returns are, say, one standard deviation away from the mean. First, we will create an indicator for the mean return with `sd(portfolio_returns_tq_rebalanced_monthly$returns)` and one for the standard deviation with `mean(portfolio_returns_tq_rebalanced_monthly$returns)`. We will call the variables `mean_plot` and `sd_plot`.

```{r}
sd_plot <- sd(portfolio_returns_tq_rebalanced_monthly$returns)
mean_plot <- mean(portfolio_returns_tq_rebalanced_monthly$returns)
```

We want to shade the scatter points according to some logic, in this case their distance from the standard deviation of returns.  Accordingly, we'll use `mutate()` to create three columns of returns...

```{r}
portfolio_returns_tq_rebalanced_monthly %>%
  mutate(hist_col_red = 
           ifelse(returns < (mean_plot - sd_plot), 
                  returns, NA),
         hist_col_green = 
           ifelse(returns > (mean_plot + sd_plot), 
                  returns, NA),
         hist_col_blue = 
           ifelse(returns > (mean_plot - sd_plot) &
                  returns < (mean_plot + sd_plot),
                  returns, NA)) %>% 
  
  ggplot(aes(x = date)) + 
  
  geom_point(aes(y = hist_col_red),
               color = "red") +
  
  geom_point(aes(y = hist_col_green),
               color = "green") +
  
  geom_point(aes(y = hist_col_blue),
               color = "blue")
```


```{r}
portfolio_returns_tq_rebalanced_monthly %>%
  mutate(hist_col_red = 
           ifelse(returns < (mean_plot - sd_plot), 
                  returns, NA),
         hist_col_green = 
           ifelse(returns > (mean_plot + sd_plot), 
                  returns, NA),
         hist_col_blue = 
           ifelse(returns > (mean_plot - sd_plot) &
                  returns < (mean_plot + sd_plot),
                  returns, NA)) %>% 
  
  ggplot(aes(x = date)) + 
  
  geom_point(aes(y = hist_col_red),
               color = "red") +
  
  geom_point(aes(y = hist_col_green),
               color = "green") +
  
  geom_point(aes(y = hist_col_blue),
               color = "blue") +
  
  geom_hline(yintercept = (mean_plot + sd_plot), color = "purple", linetype = "dotted") +
  geom_hline(yintercept = (mean_plot-sd_plot), color = "purple", linetype = "dotted") +
  scale_x_date(breaks = pretty_breaks(n = 8)) +
 ylab("percent monthly returns") +
  ggtitle("Monthly Returns Shaded by Distance from Mean")
```


This is showing us returns over time and whether they fall below or above one standard deviation from the mean.  One thing that jumps out is how many red or green circles we see after 2017. 0! That's zero monthly returns that are least one standard deviation from the mean during calendar year 2017 (Is 2017 a year of very low volatility or did we construct an amazingly stable porftolio? Check out the VIX during 2017!). When we get to rolling volatility, we should see this reflected as a low rolling volatility through 2017. If we something different, we need to investigate.


## Rolling Volatility

We have already calculated the volatility for the entire life of the portfolio but that's not quite adequate to understand the volatility for this collection of assets. 
 
We might miss a 3-month or 6-month period where the volatility spiked or plummeted or did both. And the longer our portfolio life, the more likely we are to miss something important. If we had 10 or 20 years of data and we calculated the standard deviation for the entire historyy, we could, or most certainly would, fail to notice a period in which volatility was very high, and hence we would fail to ponder the probability that it could occur again. 

Imagine a portfolio which had a standard deviation of returns for each 6-month period of 3% and it never changed. Now imagine a portfolio whose volatility fluctuated every few 6-month periods from 0% to 6% . We might find a 3% standard deviation of monthly returns over a 10-year sample for both of these, but those two portfolios are not exhibiting the same volatility. The rolling volatility of each would show us the differences and then we could hypothesize about the past causes and future probabilities for those differences. We might also want to think about dynamically rebalancing our portfolio to better manage volatility if we are seeing large spikes in the rolling windows.

Our least difficult task is calculating the rolling standard deviation of portfolio returns. 

In the `xts` world, we use a direct call to `rollapply` for this and need to choose a number of months for the rolling window. 

```{r}
window <- 6

port_rolling_sd_xts <- rollapply(portfolio_returns_xts_rebalanced_monthly,
                                 FUN = sd, 
                                 width = window) %>% na.omit()

```
The `tidyquant` packages has a nice way to apply the rolling function to data frames. We use `tq_mutate` and supply `mutate_fun = rollapply` as our mutation function argument, then `FUN = sd` as the nested function beneath it. 

```{r}
port_rolling_sd_tidy <- 
  portfolio_returns_tq_rebalanced_monthly %>% 
  tq_mutate(mutate_fun = rollapply,
            width = window,
            FUN = sd,
            col_rename = ("rolling_sd")) %>%
  select(date, rolling_sd) %>% 
  na.omit()
```

Take a quick peak to confirm consistent results. 

```{r}
head(port_rolling_sd_xts)
head(port_rolling_sd_tidy)
```



We now have an `xts` object called `port_rolling_sd_xts` and a tibble object called `port_rolling_sd_tidy`.  They contain the 6-month rolling standard deviations of portfolio.  

At the outset of this section, we opined that rolling volatilty might some insight that is obscured by the total volatility. Visualizing the rolling standard deviation should help to illuminate this. 

Let's start with the `xts` object and `highcharter`.

First, we will convert to our data to rounded percentages for ease of charting. 

```{r}
port_rolling_sd_xts_hc <- round(port_rolling_sd_xts, 4) * 100
```


```{r}
highchart(type = "stock") %>% 
    hc_title(text = "Volatility Contribution") %>%
    hc_add_series(port_rolling_sd_xts_hc) %>% 
    hc_add_theme(hc_theme_flat()) %>%
    hc_yAxis(
      labels = list(format = "{value}%"), 
             opposite = FALSE) %>%
    hc_navigator(enabled = FALSE) %>% 
    hc_scrollbar(enabled = FALSE)
```


Maybe we should add a flag to highlight this event. We can also add flags for the maximum SPY volatility, maximum and minimum portfolio rolling volatility and might as well include a line for the mean rolling volatility of SPY to practice adding horizontal lines. 

We will use two methods for adding flags. First, we'll hard code the date for the flag as "2016-04-29", which is the date when rolling SPY volatility dipped below the portfolio. 

Second, we'll set a flag with the date     
`as.Date(index(port_rolling_sd_xts_hc[which.max(port_rolling_sd_xts_hc)]),format = "%Y-%m-%d")` which looks like a convoluted mess but is adding a date for whenever the rolling portfolio standard deviation hit its maximum. 


```{r}

port_max_date <- as.Date(index(port_rolling_sd_xts_hc[which.max(port_rolling_sd_xts_hc)]),
                         format = "%Y-%m-%d")
port_min_date <- as.Date(index(port_rolling_sd_xts_hc[which.min(port_rolling_sd_xts_hc)]),
                         format = "%Y-%m-%d")


highchart(type = "stock") %>%
  hc_title(text = "Portfolio Rolling Volatility") %>%
  hc_yAxis(title = list(text = "Volatility"),
           labels = list(format = "{value}%"),
           opposite = FALSE) %>%
  hc_add_series(port_rolling_sd_xts_hc, 
                name = "Portf Volatility", 
                color = "cornflowerblue", 
                id = "Port") %>%
   hc_add_series_flags(port_max_date,
                      title = c("Max Rolling Vol"), 
                      text = c("maximum rolling volatility."),
                      id = "Port") %>%
  hc_add_series_flags(port_min_date,
                      title = c("Min Rolling Vol"), 
                      text = c("min rolling volatility."),
                      id = "Port") %>% 
  hc_navigator(enabled = FALSE) %>% 
  hc_scrollbar(enabled = FALSE)


```

Let's do the same with our data frame and ggplot. 

```{r}
port_rolling_sd_tidy %>%
  ggplot(aes(x = date)) + 
  geom_line(aes(y = rolling_sd), color = "cornflowerblue") + 
  scale_y_continuous(labels = scales::percent) +
  scale_x_date(breaks = pretty_breaks(n = 8))
```

Note that we didn't do any work to change our decimal to percentage format. When we called 
`scale_y_continuous(labels = scales::percent)` it did that work for us by adding the % sign and multiplying by 100. 

Do these visualizations add to our understanding of this portfolio? Well, we can see a spike in rolling volatility in early 2016 followed by a consistently falling vol through to mid 2017. 


It's remarkable how rolling volatility has absolutely plunged since early-to-mid 2016. 


## Shiny App

Now let's wrap all of that work into a Shiny app that allows a user to choose a 5-asset portfolio and chart rollling volatility of different widths. 

The app is availalbe online here: 

www.reproduciblefinance.com/shiny/portfolio-volatility-shiny-app/

Our input sidebar is almost identical to the previous apps on portfolio returns and dollar growth. For the full writeup on the input sidebar, see this section. 

One difference is we let the user choose a rolling window with a `numerInput()`. The code chunk below is part of our sidebar. 

```{r, eval = FALSE}
fluidRow(
  column(5,
  numericInput("window", "Window", 12, min = 3, max = 36, step = 1))
)
```

Next we lean on our previous work to calculate portfolio returns and save them as `portfolio_returns_tq_rebalanced_monthly`.  Then we pass that object on to a `dplyr` piped flow to calculate rolling standard deviation. 

```{r}
port_rolling_sd_tidy <- 
  portfolio_returns_tq_rebalanced_monthly %>% 
  tq_mutate(mutate_fun = rollapply,
            width = window,
            FUN = sd,
            col_rename = ("rolling_sd")) %>%
  select(date, rolling_sd) %>% 
  na.omit()
```

We now have the object `port_rolling_sd_tidy` available for downstream visualizations. We will start in with highcharter and that requires changing to an `xts` object first. 

```{r, eval = FALSE}
renderHighchart({
  
  port_rolling_sd_tidy_hc <- 
    port_rolling_sd_tidy() %>% 
    tk_xts(date_col = date) %>% 
    round(., 4) * 100

  
  highchart(type = "stock") %>% 
    hc_title(text = "Portfolio Rolling Volatility") %>%
    hc_yAxis(title = list(text = "Volatility"),
           labels = list(format = "{value}%"),
           opposite = FALSE) %>% 
    hc_add_series(port_rolling_sd_tidy_hc, 
                  name = "Portfolio Vol", 
                  color = "cornflowerblue",
                  id = "Port") %>%
    hc_add_theme(hc_theme_flat()) %>%
    hc_navigator(enabled = FALSE) %>% 
    hc_scrollbar(enabled = FALSE)
```

Let's also add a flag for the min and max rolling volatility. We create the flags with the following:  

```{r, eval = FALSE}
  port_max_date <- as.Date(index(port_rolling_sd_tidy_hc[which.max(port_rolling_sd_tidy_hc)]),
                         format = "%Y-%m-%d")
  port_min_date <- as.Date(index(port_rolling_sd_tidy_hc[which.min(port_rolling_sd_tidy_hc)]),
                         format = "%Y-%m-%d")
```


Then add to the chart with the two `hc_add_series_flags()` function calls below: 

```{r, eval = FALSE}

   hc_add_series_flags(port_max_date,
                      title = c("Max Rolling Vol"), 
                      text = c("maximum rolling volatility."),
                      id = "Port") %>%
  hc_add_series_flags(port_min_date,
                      title = c("Min Rolling Vol"), 
                      text = c("min rolling volatility."),
                      id = "Port")
```


Now on to `ggplot()`. We don't need to alter the original `port_rolling_sd_tidy` reactive and can pass it straight into a piped `dplyr/ggplot` flow. 

```{r, eval = FALSE}
renderPlot({
  port_rolling_sd_tidy() %>% 
    ggplot(aes(x = date)) +
    geom_line(aes(y = rolling_sd), color = "cornflowerblue") + 
    scale_y_continuous(labels = scales::percent) +
    ggtitle("Portfolio Rolling Vol") +
    ylab("volatility") +
    scale_x_date(breaks = pretty_breaks(n = 8)) +
    theme(plot.title = element_text(hjust = 0.5))
})
```



## Component Contribution to Volatility

Now let's break that total portfolio volatility into its constituent parts and investigate how each asset contributes to the volatility. Why might we want to do that?

For our own risk management purposes, we might want to ensure that our risk hasn't got too concentrated in one asset. Not only might this lead a less diversified portfolio than we thought we had, but it also might indicate that our initial assumptions about a particular asset were wrong, or at least, they have become less right as the asset has changed over time. 

Similarly, if this portfolio is governed by a mandate from, say, an institutional client, that client might have a preference or even a rule that no asset or sector can rise above a certain threshold risk contribution. That institutional client might require a report like this from each of their outsourced managers, so they can sum the constituents.  
 
As in the previous section on volatility, we need to build the covariance matrix and calculate portfolio standard deviation.

```{r, message = FALSE}
covariance_matrix <- cov(asset_returns_xts)

# Square root of transpose of the weights cross prod covariance matrix returns 
# cross prod weights gives portfolio standard deviation.
sd_portfolio <- sqrt(t(w) %*% covariance_matrix %*% w)
```

Now let's start to look at the individual components.

The percentage contribution of asset i is defined as:
(marginal contribution of asset i * weight of asset i) / portfolio standard deviation

Let's find the marginal contribution first by taking the cross product of the weights vector and the covariance matrix, divided by the portfolio standard deviation.

```{r}

# Marginal contribution of each asset. 
marginal_contribution <- w %*% covariance_matrix / sd_portfolio[1, 1]

marginal_contribution
```

Now multiply the marginal contribution of each asset by the weights vector to get total contribution. 


```{r}
component_contribution <- marginal_contribution * w 

component_contribution
```

We can then sum the asset contributions and make sure it's equal to total portfolio standard deviation.

```{r}

components_summed <- rowSums(component_contribution)

components_summed
sd_portfolio
```

The summed components are equal to the total - all looks good.

To get to percentage contribution of each asset, we divide each asset's contribution by total portfolio standard deviation.

```{r}
component_percentages <- component_contribution / sd_portfolio[1, 1]

component_percentages
```

Let's port this to a tibble for ease of presentation, and we'll append `by_hand` to the object because we did the calculations step-by-step.

```{r}
percentage_tibble_by_hand <- 
  tibble(symbols, w, as.vector(component_percentages)) %>% 
  rename(asset = symbols, 
         'portfolio weight' = w, 
         'risk contribution' = `as.vector(component_percentages)`) %>% 
  mutate(`risk contribution` = round(`risk contribution`, 4) * 100)

percentage_tibble_by_hand
```

Does anything strike us as out of whack here? Is AGG acting as the volatility dampener we had hoped? 

As you might have guessed, we used `by_hand` in the object name because we could have used a pre-built R function to do all this work.

The `StdDev()` function from `PerformanceAnalytics` will run this same component calculation if we pass in an asset returns object, weights vector and then set `portfolio_method = "component"` (recall that if we set `portfolio_method = "single"`, the function will return the total portfolio standard deviation, as we saw in a previous section).

Let's confirm that the pre-built function returns the same results.

```{r}

portfolio_vol_comp_contr_total_builtin <- StdDev(asset_returns_xts, weights = w, 
                              portfolio_method = "component")
portfolio_vol_comp_contr_total_builtin
```

That function returns a list and one of the elements is `$pct_contrib_StdDev`, which is the percentage contribution of each asset. Let's move it to a `tibble` for ease of presentation.

```{r}
# Port to a tibble.  
percentages_tibble_pre_built <- 
  portfolio_vol_comp_contr_total_builtin$pct_contrib_StdDev %>%
  tk_tbl(preserve_index = FALSE) %>%
  mutate(asset = symbols) %>%
  rename('risk contribution' = data) %>%
  mutate(`risk contribution` = round(`risk contribution`, 4) * 100, 
         weights = w *100 ) %>% 
  select(asset, everything())
```

Has our work checked out? Is `percentages_tibble_pre_built` showing the same result as `component_percentages_tibble_by_hand`? 

Compare the two objects

```{r, message=FALSE}
percentages_tibble_pre_built
percentage_tibble_by_hand
```

Huzzah - our findings seem to be consistent! 

## Visualizing component contribution to risk

Our substantive work is done but let's turn to `ggplot` for some visualizing. We have not yet built a bar chart

```{r}
percentages_tibble_pre_built %>% 
  mutate(`risk contribution` = `risk contribution`/100) %>% 
  ggplot(aes(x = asset, y = `risk contribution`)) +
  geom_col(fill = 'cornflowerblue', colour = 'pink', width = .6) + 
  scale_y_continuous(labels = scales::percent, breaks = pretty_breaks(n = 20)) + 
  ggtitle("Percent Contribution to Volatility") +
  theme(plot.title = element_text(hjust = 0.5)) +
  xlab("Asset") +
  ylab("Percent Contribution to Risk")

```

How about a chart that compares weights to risk contribution. First we'll need to gather our tibble to long format, then call `ggplot`.

```{r}

  percentages_tibble_pre_built %>% 
  gather(type, percent, -asset) %>% 
  group_by(type) %>% 
  mutate(percent = percent/100) %>% 
  ggplot(aes(x = asset, y = percent, fill = type)) +
  geom_col(position='dodge') + 
  scale_y_continuous(labels = percent) + 
  ggtitle("Percent Contribution to Volatility") +
  theme(plot.title = element_text(hjust = 0.5))

```

It looks like AGG, a bond fund, has done a good job as a volatility dampener. It has a 10% allocation but contributes almost zero to volatility. We're ignoring returns in this section and focusing on risk. If AGG is crushing our returns, we may not be thrilled even with its risk softening prowess.


## Rolling Component Contribution

Now we turn to our most involved task yet, to calculate the rolling volatility contribution of each asset.  The previous section told us the total contribution of each asset over the life of the portfolio, but it did not help us understand risk components over time. As we discussed in our section on rolling portfolio volatility, there's reasons to care about rolling. 

This task had the potential to be quite straightforward with the following. 

```{r}
                      
rolling_contribution <- rollapply(asset_returns_xts, window, 
                           function(x) StdDev(x, weights = w))
```

That code throws an error that the weights vector doesn't play nicely with `rollapply()`. There might be a hack around this problem but let's not look for the hack and take this as a good chance to tackle a problem where the built-in functions accomplish 90% of our task but we need to construct code for the final 10%.  Our previous work manipulating objects from data frames to `xts` and slicing/dicing will serve us well here. 

Our goal is to create a function that takes (1) a `data.frame` of asset returns and calculates the rolling contributions of each asset to volatility, based on a (2) starting date index and a (3) window, for a portfolio (4) with specified weights of each asset.  We will need to supply four arguments to the function, accordingly.

We will get to the function in all its glory below, but before that here is the logic to construct that function (feel free to eviscerate this logic and replace it with something better).  

1. Assign a start date for the first rolling calculation.
2. Assign end date based on the start date and window argument. If we set window = 6, we'll be calculating contributions for the 6-month period between start date and end date. 
3. Use `filter()` to subset the original `data.frame` down to one window. I label the subsetted data frame as `interval_to_use`. In our example, that interval is a 6-month window of our original data frame. 
4. Now we want to pass that `interval_to_use` object to `StdDev()` but it's not an `xts` object. We need to convert it and label it `returns_xts`. 
5. Before we call `StdDev()`, we need weights. Create a weights object called `w` and assign the value from the argument we supplied to the function.
6. Pass the `returns_xts` and `w` to `StdDev()`, and set `portfolio_method = "component"`.
7. We now have an object called `results_as_xts`. What is this? It's the risk contributions for each asset during the first 6-month window of our weighted portfolio. If our data starts at February, 2013, we now have one calculation for the period from February to August.
8. Convert it back to a `tibble` and return.
9. We now have the risk contributions for the 6-month period that started on the first date or February 2013, because we default to `start = 1`. If we wanted to get the standard deviation for a 6-month period that started on the second date or March 2013, we could set `start = 2`, etc. 
10. If we want risk contribution for all the 6-month periods, we need to apply this function starting at February, then March, then April, all the way to the month that is six months before the end of our data.

Now, let's turn those enumerated steps into a function.

```{r SD Interval Function, message = FALSE}

my_interval_sd <- function(returns_df, start = 1, window = 6, weights){
  
  # First create start date. 
  start_date <- returns_df$date[start]
  
  # Next an end date that depends on start date and window.
  end_date <-  returns_df$date[c(start + window)]
  
  # Filter on start and end date.
  interval_to_use <- returns_df %>% filter(date >= start_date & date < end_date)
  
  # Convert to xts so can use built in Performance Analytics function.
  returns_xts <- interval_to_use %>% tk_xts(date_var = date) 
  
  # Portfolio weights.
  w <- weights
  
  # Pass xts object to function.
  results_as_xts <- StdDev(returns_xts, weights = w, portfolio_method = "component")
  
  # Convert results to tibble with a nicely formatted date.
  results_to_tibble <- tk_tbl(t(results_as_xts$pct_contrib_StdDev)) %>% 
    mutate(date = ymd(end_date)) %>% 
    select(date, everything()) 
}
```

Let's apply that function to a returns object to see the results. 

```{r}
test_my_function <- my_interval_sd(asset_returns_dplyr_byhand, start = 1, window = 6, weights = w)

test_my_function
```

The function returns something, but what? It's showing us the contribution to risk for each asset from February through August. The reason the date column is `2013-08-30` is that as of that date, this is the contribution over the previous 6 months of each asset. 

What if we started at `start = 2`? 

```{r}

test_my_function_2 <- my_interval_sd(asset_returns_dplyr_byhand, start = 2, window = 6, weights = w)
test_my_function_2

```

We have moved up by one month to September. 

We could keep doing this - applying our function to different start dates until we got to the end of our data. But instead we'll use `map_df()` to apply the function to our vector of dates. 

We will invoke `map_df()` to apply our function to date 1, then save the result to a `data.frame`, then apply our function to date 2, and save to that same `data.frame`, and so on until we tell it stop at the at index that is 6 months before the last date index.

```{r, message=FALSE, warning=FALSE}
window <- 6

portfolio_vol_components_tidy <- 
  map_df(1:(nrow(asset_returns_dplyr_byhand)-window), 
         my_interval_sd, 
         returns_df = asset_returns_dplyr_byhand, 
         weights = w, window = window) %>%
  mutate_if(is.numeric, funs(round(., 3) * 100))

portfolio_vol_components_tidy

```


```{r}
window <- 6

portfolio_vol_components_tidy <- 
    map_df(1:(nrow(asset_returns_dplyr_byhand)-window), 
           my_interval_sd, returns_df = asset_returns_dplyr_byhand, 
           weights = w, window = window) %>%
    mutate_all(funs(round(., 3))) %>% 
    mutate(date = ymd(date)) %>%
    select(date, everything())

portfolio_vol_components_tidy
```


Now we can visualize with `highcharter` by adding the rolling conbtribution of each asset to a chart. We are going to tweak the minimum and maximum y-axis values by adding `hc_yAxis(...max = max(portfolio_vol_components_tidy_xts) + 5, min = min(portfolio_vol_components_tidy_xts) - 5`. That line of code will set the maximum y-axis value to the highest value in our data + 5 and the minimum y-axis value to the lowest value in our data - 5. This is purely aesthetic to make the chart a bit more readable.

```{r}
portfolio_vol_components_tidy_xts <- 
  portfolio_vol_components_tidy %>%
  mutate_if(is.numeric, funs(. * 100)) %>% 
  tk_xts(date_var = date)


  highchart(type = "stock") %>% 
  hc_title(text = "Volatility Contribution") %>%
  hc_add_series(portfolio_vol_components_tidy_xts[, 1], name = symbols[1], id = "SPY") %>%
  hc_add_series(portfolio_vol_components_tidy_xts[, 2], name = symbols[2]) %>%
  hc_add_series(portfolio_vol_components_tidy_xts[, 3], name = symbols[3] )%>%
  hc_add_series(portfolio_vol_components_tidy_xts[, 4], name = symbols[4]) %>%
  hc_add_series(portfolio_vol_components_tidy_xts[, 5], name = symbols[5]) %>%
  hc_yAxis(labels = list(format = "{value}%"), 
           max = max(portfolio_vol_components_tidy_xts) + 5,
           min = min(portfolio_vol_components_tidy_xts) - 5,
           opposite = FALSE) %>%
  hc_navigator(enabled = FALSE) %>% 
  hc_scrollbar(enabled = FALSE)

```

For some reason, EEM rolling contribution spiked in June/July of 2017. Did something roil emerging markets in the preceding months? And IJS plunged. Something looks strange here. 

As per our usual, let's head to `ggplot()` and recreate that chart. Since our data object is still in wide format, we will need to make it long/tidy with `gather(asset, contribution, -date)`, then we can chart by asset.

```{r}

  portfolio_vol_components_tidy %>% 
    gather(asset, contribution, -date) %>% 
    group_by(asset) %>%
    ggplot(aes(x = date)) +
    geom_line(aes(y = contribution, color = asset)) +
    scale_y_continuous(labels = percent)

```

We have painted a picture of the rolling contribution over time and used bar charts to show contribution versus weights during the life of the portfolio. Let's port this over Shiny.

Have a look at the finished app here: 

www.reproduciblefinance.com/shiny/volatility-contribution/

Note that we are including 4 data visualizations from our previous work.
1) a bar chart of asset contribution to volatility
2) a bar chart of asset weight and contribution
3) a highchart of rolling asset contribution
4) a `ggplot` of rolling asset contribution

Let's get started.

By now you're probably tired of hearing that our input sidebar will let the user choose stocks, weights and a start date, plus a rolling window. If that sidebar is now boringly easy to build, that's okay and, in fact, maybe it's fantastic. Setting up the data selection aspect of portfolio Shiny apps is starting to feel like second nature. 

We are performing more involved calculations in this app and using our own function `my_interval_sd()`. We can define it in a separate code chunk from the input sidebar.


```{r, eval = FALSE}
my_interval_sd <- function(returns_df, start = 1, weights, window = 20){
  
  # First create start date
  start_date = returns_df$date[start]
  
  # Next an end date
  end_date = returns_df$date[c(start + window)]
  
  # Filter on start and end date
  interval_to_use <- returns_df %>% filter(date >= start_date & date < end_date)
  
  # Convert to xts so can use built in Performance Analytics function.
  returns_xts <- interval_to_use %>% tk_xts(date_var = date) 
  
  # Portfolio weights.
  w <- weights
  
  # Pass xts object to function.
  results_as_xts <- StdDev(returns_xts, weights = w, portfolio_method = "component")
  
  # Convert results to tibble.
  results_to_tibble <- tk_tbl(t(results_as_xts$pct_contrib_StdDev)) %>% 
    mutate(date = ymd(end_date)) %>%
    mutate_if(is.numeric, function(x) x * 100) %>% 
    select(date, everything()) 
}
```


```{r, eval = FALSE}
asset_returns_dplyr_byhand <- eventReactive(input$go, {
  
  symbols <- c(input$stock1, input$stock2, input$stock3, input$stock4, input$stock5)
  
  prices <- 
    getSymbols(symbols, src = 'yahoo', from = input$date, 
               auto.assign = TRUE, warnings = FALSE) %>% 
    map(~Ad(get(.))) %>% 
    reduce(merge) %>%
    `colnames<-`(symbols)

  
  asset_returns_dplyr_byhand <- 
    prices %>% 
    to.monthly(indexAt = "last", OHLC = FALSE) %>% 
    tk_tbl(preserve_index = TRUE, rename_index = "date") %>%
    gather(asset, returns, -date) %>%
    group_by(asset) %>% 
    mutate(returns = (log(returns) - log(lag(returns)))) %>%
    spread(asset, returns) %>% 
    select(date, symbols) %>% 
    slice(-1)
})

percentages_tibble_pre_built <- eventReactive(input$go, {
  
  asset_returns_xts <- 
    asset_returns_dplyr_byhand() %>% 
    tk_xts(date_col = date)
  
  w <- c(input$w1/100, input$w2/100, input$w3/100, input$w4/100, input$w5/100)
  
  portfolio_vol_comp_contr_total_builtin <- 
    StdDev(asset_returns_xts, 
           weights = w,
           portfolio_method = "component")
  
  symbols <- c(input$stock1, input$stock2, input$stock3, input$stock4, input$stock5)
  
  percentages_tibble_pre_built <- 
    portfolio_vol_comp_contr_total_builtin$pct_contrib_StdDev %>%
    tk_tbl(preserve_index = FALSE) %>%
    mutate(asset = symbols) %>%
    rename('risk contribution' = data) %>%
    mutate(`risk contribution` = round(`risk contribution`, 4) * 100, 
           weights = w * 100) %>% 
    select(asset, everything())
  
})

portfolio_vol_components_tidy <- eventReactive(input$go, {
  
  asset_returns_dplyr_byhand <- asset_returns_dplyr_byhand()
  
  w <- c(input$w1/100, input$w2/100, input$w3/100, input$w4/100, input$w5/100)
  
   port_components_tidy <- 
    map_df(1:(nrow(asset_returns_dplyr_byhand)-input$window), 
           my_interval_sd, returns_df = asset_returns_dplyr_byhand, 
           weights = w, window = input$window) %>%
    mutate_all(funs(round(., 3))) %>% 
    mutate(date = ymd(date)) %>%
    select(date, everything()) 
})
```


After the input sidebar, we have some complex functions to house. 


